{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tom\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "voices = engine.getProperty('voices') \n",
    "engine.setProperty('voice', voices[1].id)\n",
    "rate = engine.getProperty('rate')\n",
    "engine.setProperty('rate', 115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talk\n",
      "Time over, thanks\n",
      "Sorry, I did not get that\n"
     ]
    }
   ],
   "source": [
    "# Initialize recognizer class (for recognizing the speech)\n",
    "\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Reading Microphone as source\n",
    "# listening the speech and store in audio_text variable\n",
    "\n",
    "with sr.Microphone() as source:\n",
    "    r.adjust_for_ambient_noise(source, duration=5)\n",
    "    print(\"Talk\")\n",
    "    audio_text = r.listen(source)\n",
    "    print(\"Time over, thanks\")\n",
    "# recoginize_() method will throw a request error if the API is unreachable, hence using exception handling\n",
    "    \n",
    "    try:\n",
    "        # using google speech recognition\n",
    "        print(\"Text: \"+r.recognize_google(audio_text))\n",
    "    except:\n",
    "         print(\"Sorry, I did not get that\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "with open(r\"C:\\Users\\Tom\\OneDrive\\Desktop\\Git Repository\\math_project\\website\\api\\intents_ancient.json\") as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "\n",
    "    if intent[\"tag\"] not in labels:\n",
    "        labels.append(intent[\"tag\"])\n",
    "\n",
    "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "labels = sorted(labels)\n",
    "\n",
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "\n",
    "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "\n",
    "\n",
    "training = numpy.array(training)\n",
    "output = numpy.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tom\\anaconda3\\lib\\site-packages\\tflearn\\initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#tensorflow.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: 0P0ZN8\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 8\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.083s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.98879\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 002 | loss: 0.98879 - acc: 0.3375 -- iter: 8/8\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m1.07850\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 003 | loss: 1.07850 - acc: 0.3682 -- iter: 8/8\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m1.09331\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 004 | loss: 1.09331 - acc: 0.3733 -- iter: 8/8\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m1.09659\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 005 | loss: 1.09659 - acc: 0.3745 -- iter: 8/8\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m1.09740\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 006 | loss: 1.09740 - acc: 0.3748 -- iter: 8/8\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m1.09755\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 007 | loss: 1.09755 - acc: 0.3749 -- iter: 8/8\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m1.09750\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 008 | loss: 1.09750 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m1.09736\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 009 | loss: 1.09736 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m1.09719\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 010 | loss: 1.09719 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m1.09701\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 011 | loss: 1.09701 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m1.09682\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 012 | loss: 1.09682 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m1.09661\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 013 | loss: 1.09661 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m1.09640\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 014 | loss: 1.09640 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m1.09618\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 015 | loss: 1.09618 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m1.09595\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 016 | loss: 1.09595 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m1.09570\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 017 | loss: 1.09570 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m1.09545\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 018 | loss: 1.09545 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m1.09518\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 019 | loss: 1.09518 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m1.09491\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 020 | loss: 1.09491 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m1.09461\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 021 | loss: 1.09461 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m1.09430\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 022 | loss: 1.09430 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m1.09397\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 023 | loss: 1.09397 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m1.09363\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 024 | loss: 1.09363 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m1.09326\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 025 | loss: 1.09326 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m1.09288\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 026 | loss: 1.09288 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m1.09247\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 027 | loss: 1.09247 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m1.09204\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 028 | loss: 1.09204 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m1.09158\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 029 | loss: 1.09158 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m1.09109\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 030 | loss: 1.09109 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m1.09058\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 031 | loss: 1.09058 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m1.09003\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 032 | loss: 1.09003 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m1.08946\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 033 | loss: 1.08946 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m1.08884\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 034 | loss: 1.08884 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m1.08819\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 035 | loss: 1.08819 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m1.08751\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 036 | loss: 1.08751 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m1.08678\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 037 | loss: 1.08678 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m1.08601\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 038 | loss: 1.08601 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m1.08520\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 039 | loss: 1.08520 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m1.08434\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 040 | loss: 1.08434 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m1.08344\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 041 | loss: 1.08344 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m1.08248\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 042 | loss: 1.08248 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m1.08147\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 043 | loss: 1.08147 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m1.08040\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 044 | loss: 1.08040 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m1.07928\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 045 | loss: 1.07928 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m1.07811\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 046 | loss: 1.07811 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m1.07687\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 047 | loss: 1.07687 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m1.07556\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 048 | loss: 1.07556 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m1.07420\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 049 | loss: 1.07420 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m1.07276\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 050 | loss: 1.07276 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m1.07126\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 051 | loss: 1.07126 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m1.06969\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 052 | loss: 1.06969 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m1.06804\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 053 | loss: 1.06804 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m1.06632\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 054 | loss: 1.06632 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m1.06453\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 055 | loss: 1.06453 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m1.06266\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 056 | loss: 1.06266 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m1.06071\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 057 | loss: 1.06071 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m1.05868\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 058 | loss: 1.05868 - acc: 0.3750 -- iter: 8/8\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m1.05657\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 059 | loss: 1.05657 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m1.05438\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 060 | loss: 1.05438 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m1.05210\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 061 | loss: 1.05210 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m1.04974\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 062 | loss: 1.04974 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m1.04730\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 063 | loss: 1.04730 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m1.04477\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 064 | loss: 1.04477 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m1.04215\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 065 | loss: 1.04215 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m1.03944\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 066 | loss: 1.03944 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m1.03665\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 067 | loss: 1.03665 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m1.03377\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 068 | loss: 1.03377 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m1.03080\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 069 | loss: 1.03080 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m1.02775\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 070 | loss: 1.02775 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m1.02460\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 071 | loss: 1.02460 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m1.02137\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 072 | loss: 1.02137 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m1.01804\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 073 | loss: 1.01804 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m1.01463\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 074 | loss: 1.01463 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m1.01112\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 075 | loss: 1.01112 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m1.00753\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 076 | loss: 1.00753 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m1.00384\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 077 | loss: 1.00384 - acc: 0.3750 -- iter: 8/8\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m1.00006\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 078 | loss: 1.00006 - acc: 0.3881 -- iter: 8/8\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.99618\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 079 | loss: 0.99618 - acc: 0.4126 -- iter: 8/8\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.99221\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 080 | loss: 0.99221 - acc: 0.4343 -- iter: 8/8\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.98814\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 081 | loss: 0.98814 - acc: 0.4536 -- iter: 8/8\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.98398\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 082 | loss: 0.98398 - acc: 0.4707 -- iter: 8/8\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.97967\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 083 | loss: 0.97967 - acc: 0.4862 -- iter: 8/8\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.97522\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 084 | loss: 0.97522 - acc: 0.5000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.97063\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 085 | loss: 0.97063 - acc: 0.5125 -- iter: 8/8\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.96590\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 086 | loss: 0.96590 - acc: 0.5238 -- iter: 8/8\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.96103\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 087 | loss: 0.96103 - acc: 0.5339 -- iter: 8/8\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.95602\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 088 | loss: 0.95602 - acc: 0.5430 -- iter: 8/8\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.95088\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 089 | loss: 0.95088 - acc: 0.5512 -- iter: 8/8\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.94560\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 090 | loss: 0.94560 - acc: 0.5586 -- iter: 8/8\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.94020\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 091 | loss: 0.94020 - acc: 0.5652 -- iter: 8/8\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.93466\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 092 | loss: 0.93466 - acc: 0.5712 -- iter: 8/8\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.92900\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 093 | loss: 0.92900 - acc: 0.5766 -- iter: 8/8\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.92320\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 094 | loss: 0.92320 - acc: 0.5814 -- iter: 8/8\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.91729\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 095 | loss: 0.91729 - acc: 0.5858 -- iter: 8/8\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.91125\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 096 | loss: 0.91125 - acc: 0.5897 -- iter: 8/8\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.90509\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 097 | loss: 0.90509 - acc: 0.5932 -- iter: 8/8\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.89882\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 098 | loss: 0.89882 - acc: 0.5964 -- iter: 8/8\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.89243\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 099 | loss: 0.89243 - acc: 0.5993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.88593\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 100 | loss: 0.88593 - acc: 0.6018 -- iter: 8/8\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.87932\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 101 | loss: 0.87932 - acc: 0.6042 -- iter: 8/8\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.87260\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 102 | loss: 0.87260 - acc: 0.6062 -- iter: 8/8\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.86578\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 103 | loss: 0.86578 - acc: 0.6081 -- iter: 8/8\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.85886\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 104 | loss: 0.85886 - acc: 0.6098 -- iter: 8/8\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.85185\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 105 | loss: 0.85185 - acc: 0.6238 -- iter: 8/8\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.87215\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 106 | loss: 0.87215 - acc: 0.5989 -- iter: 8/8\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.86233\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 107 | loss: 0.86233 - acc: 0.6141 -- iter: 8/8\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.85279\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 108 | loss: 0.85279 - acc: 0.6276 -- iter: 8/8\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.84349\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 109 | loss: 0.84349 - acc: 0.6399 -- iter: 8/8\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.83439\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 110 | loss: 0.83439 - acc: 0.6634 -- iter: 8/8\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.82548\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 111 | loss: 0.82548 - acc: 0.6846 -- iter: 8/8\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.81671\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 112 | loss: 0.81671 - acc: 0.7036 -- iter: 8/8\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.80808\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 113 | loss: 0.80808 - acc: 0.7207 -- iter: 8/8\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.79956\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 114 | loss: 0.79956 - acc: 0.7487 -- iter: 8/8\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.79114\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 115 | loss: 0.79114 - acc: 0.7738 -- iter: 8/8\n",
      "--\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.78281\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 116 | loss: 0.78281 - acc: 0.7964 -- iter: 8/8\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.77454\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 117 | loss: 0.77454 - acc: 0.8168 -- iter: 8/8\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.83933\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 118 | loss: 0.83933 - acc: 0.7601 -- iter: 8/8\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.82412\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 119 | loss: 0.82412 - acc: 0.7841 -- iter: 8/8\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.80988\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 120 | loss: 0.80988 - acc: 0.8057 -- iter: 8/8\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.79649\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 121 | loss: 0.79649 - acc: 0.8251 -- iter: 8/8\n",
      "--\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.78385\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 122 | loss: 0.78385 - acc: 0.8426 -- iter: 8/8\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.77188\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 123 | loss: 0.77188 - acc: 0.8583 -- iter: 8/8\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.76048\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 124 | loss: 0.76048 - acc: 0.8725 -- iter: 8/8\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.74960\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 125 | loss: 0.74960 - acc: 0.8853 -- iter: 8/8\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.73918\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 126 | loss: 0.73918 - acc: 0.8967 -- iter: 8/8\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.72915\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 127 | loss: 0.72915 - acc: 0.9071 -- iter: 8/8\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.71949\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 128 | loss: 0.71949 - acc: 0.9164 -- iter: 8/8\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.71014\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 129 | loss: 0.71014 - acc: 0.9247 -- iter: 8/8\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.70108\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 130 | loss: 0.70108 - acc: 0.9322 -- iter: 8/8\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.69226\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 131 | loss: 0.69226 - acc: 0.9390 -- iter: 8/8\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.68368\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 132 | loss: 0.68368 - acc: 0.9451 -- iter: 8/8\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.67529\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 133 | loss: 0.67529 - acc: 0.9506 -- iter: 8/8\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.66710\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 134 | loss: 0.66710 - acc: 0.9555 -- iter: 8/8\n",
      "--\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.65906\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 135 | loss: 0.65906 - acc: 0.9600 -- iter: 8/8\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.65118\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 136 | loss: 0.65118 - acc: 0.9640 -- iter: 8/8\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.64344\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 137 | loss: 0.64344 - acc: 0.9676 -- iter: 8/8\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.63582\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 138 | loss: 0.63582 - acc: 0.9708 -- iter: 8/8\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.62832\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 139 | loss: 0.62832 - acc: 0.9738 -- iter: 8/8\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.62092\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 140 | loss: 0.62092 - acc: 0.9764 -- iter: 8/8\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.61363\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 141 | loss: 0.61363 - acc: 0.9787 -- iter: 8/8\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.60643\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 142 | loss: 0.60643 - acc: 0.9809 -- iter: 8/8\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.59932\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 143 | loss: 0.59932 - acc: 0.9828 -- iter: 8/8\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.59230\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 144 | loss: 0.59230 - acc: 0.9845 -- iter: 8/8\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.58535\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 145 | loss: 0.58535 - acc: 0.9860 -- iter: 8/8\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.71323\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 146 | loss: 0.71323 - acc: 0.9124 -- iter: 8/8\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.69318\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 147 | loss: 0.69318 - acc: 0.9212 -- iter: 8/8\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.67472\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 148 | loss: 0.67472 - acc: 0.9291 -- iter: 8/8\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.65769\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 149 | loss: 0.65769 - acc: 0.9362 -- iter: 8/8\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.64192\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 150 | loss: 0.64192 - acc: 0.9426 -- iter: 8/8\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.62727\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 151 | loss: 0.62727 - acc: 0.9483 -- iter: 8/8\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.61362\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 152 | loss: 0.61362 - acc: 0.9535 -- iter: 8/8\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.60085\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 153 | loss: 0.60085 - acc: 0.9581 -- iter: 8/8\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.58888\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 154 | loss: 0.58888 - acc: 0.9623 -- iter: 8/8\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.57761\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 155 | loss: 0.57761 - acc: 0.9661 -- iter: 8/8\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.56696\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 156 | loss: 0.56696 - acc: 0.9695 -- iter: 8/8\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.55686\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 157 | loss: 0.55686 - acc: 0.9725 -- iter: 8/8\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.54726\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 158 | loss: 0.54726 - acc: 0.9753 -- iter: 8/8\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.53810\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 159 | loss: 0.53810 - acc: 0.9777 -- iter: 8/8\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.52932\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 160 | loss: 0.52932 - acc: 0.9800 -- iter: 8/8\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.52090\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 161 | loss: 0.52090 - acc: 0.9820 -- iter: 8/8\n",
      "--\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.51279\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 162 | loss: 0.51279 - acc: 0.9838 -- iter: 8/8\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.50495\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 163 | loss: 0.50495 - acc: 0.9854 -- iter: 8/8\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.49736\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 164 | loss: 0.49736 - acc: 0.9869 -- iter: 8/8\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.49000\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 165 | loss: 0.49000 - acc: 0.9882 -- iter: 8/8\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.48283\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 166 | loss: 0.48283 - acc: 0.9894 -- iter: 8/8\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.47584\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 167 | loss: 0.47584 - acc: 0.9904 -- iter: 8/8\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.46901\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 168 | loss: 0.46901 - acc: 0.9914 -- iter: 8/8\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.46233\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 169 | loss: 0.46233 - acc: 0.9922 -- iter: 8/8\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.45578\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 170 | loss: 0.45578 - acc: 0.9930 -- iter: 8/8\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.44935\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 171 | loss: 0.44935 - acc: 0.9937 -- iter: 8/8\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.44303\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 172 | loss: 0.44303 - acc: 0.9943 -- iter: 8/8\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.43681\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 173 | loss: 0.43681 - acc: 0.9949 -- iter: 8/8\n",
      "--\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.43068\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 174 | loss: 0.43068 - acc: 0.9954 -- iter: 8/8\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.42464\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 175 | loss: 0.42464 - acc: 0.9959 -- iter: 8/8\n",
      "--\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.41867\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 176 | loss: 0.41867 - acc: 0.9963 -- iter: 8/8\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.41278\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 177 | loss: 0.41278 - acc: 0.9967 -- iter: 8/8\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.40696\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 178 | loss: 0.40696 - acc: 0.9970 -- iter: 8/8\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.40120\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 179 | loss: 0.40120 - acc: 0.9973 -- iter: 8/8\n",
      "--\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.39550\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 180 | loss: 0.39550 - acc: 0.9976 -- iter: 8/8\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.38986\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 181 | loss: 0.38986 - acc: 0.9978 -- iter: 8/8\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.38427\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 182 | loss: 0.38427 - acc: 0.9980 -- iter: 8/8\n",
      "--\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.37874\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 183 | loss: 0.37874 - acc: 0.9982 -- iter: 8/8\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.37326\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 184 | loss: 0.37326 - acc: 0.9984 -- iter: 8/8\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.36782\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 185 | loss: 0.36782 - acc: 0.9986 -- iter: 8/8\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.36244\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 186 | loss: 0.36244 - acc: 0.9987 -- iter: 8/8\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.35711\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 187 | loss: 0.35711 - acc: 0.9988 -- iter: 8/8\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.35182\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 188 | loss: 0.35182 - acc: 0.9990 -- iter: 8/8\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.34658\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 189 | loss: 0.34658 - acc: 0.9991 -- iter: 8/8\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.34138\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 190 | loss: 0.34138 - acc: 0.9992 -- iter: 8/8\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.33623\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 191 | loss: 0.33623 - acc: 0.9992 -- iter: 8/8\n",
      "--\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.33113\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 192 | loss: 0.33113 - acc: 0.9993 -- iter: 8/8\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.32607\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 193 | loss: 0.32607 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.32105\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 194 | loss: 0.32105 - acc: 0.9994 -- iter: 8/8\n",
      "--\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.31609\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 195 | loss: 0.31609 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.31117\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 196 | loss: 0.31117 - acc: 0.9995 -- iter: 8/8\n",
      "--\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.30629\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 197 | loss: 0.30629 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.30146\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 198 | loss: 0.30146 - acc: 0.9996 -- iter: 8/8\n",
      "--\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.29668\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 199 | loss: 0.29668 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.28727\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 200 | loss: 0.28727 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.28727\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 201 | loss: 0.28727 - acc: 0.9997 -- iter: 8/8\n",
      "--\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.28263\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 202 | loss: 0.28263 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.27805\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 203 | loss: 0.27805 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.27351\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 204 | loss: 0.27351 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.26903\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 205 | loss: 0.26903 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.26459\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 206 | loss: 0.26459 - acc: 0.9998 -- iter: 8/8\n",
      "--\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.26021\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 207 | loss: 0.26021 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.25588\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 208 | loss: 0.25588 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.25160\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 209 | loss: 0.25160 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.24737\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 210 | loss: 0.24737 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.24320\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 211 | loss: 0.24320 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.23908\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 212 | loss: 0.23908 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.23501\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 213 | loss: 0.23501 - acc: 0.9999 -- iter: 8/8\n",
      "--\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.43847\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 214 | loss: 0.43847 - acc: 0.9374 -- iter: 8/8\n",
      "--\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.41391\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 215 | loss: 0.41391 - acc: 0.9437 -- iter: 8/8\n",
      "--\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.39159\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 216 | loss: 0.39159 - acc: 0.9493 -- iter: 8/8\n",
      "--\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.37128\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 217 | loss: 0.37128 - acc: 0.9544 -- iter: 8/8\n",
      "--\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.35278\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 218 | loss: 0.35278 - acc: 0.9589 -- iter: 8/8\n",
      "--\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.33589\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 219 | loss: 0.33589 - acc: 0.9631 -- iter: 8/8\n",
      "--\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.32046\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 220 | loss: 0.32046 - acc: 0.9667 -- iter: 8/8\n",
      "--\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.30633\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 221 | loss: 0.30633 - acc: 0.9701 -- iter: 8/8\n",
      "--\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.29337\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 222 | loss: 0.29337 - acc: 0.9731 -- iter: 8/8\n",
      "--\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.28146\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 223 | loss: 0.28146 - acc: 0.9758 -- iter: 8/8\n",
      "--\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.27050\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 224 | loss: 0.27050 - acc: 0.9782 -- iter: 8/8\n",
      "--\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.26039\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 225 | loss: 0.26039 - acc: 0.9804 -- iter: 8/8\n",
      "--\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.25104\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 226 | loss: 0.25104 - acc: 0.9823 -- iter: 8/8\n",
      "--\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.24238\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 227 | loss: 0.24238 - acc: 0.9841 -- iter: 8/8\n",
      "--\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.36096\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 228 | loss: 0.36096 - acc: 0.9482 -- iter: 8/8\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.34090\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 229 | loss: 0.34090 - acc: 0.9534 -- iter: 8/8\n",
      "--\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.61494\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 230 | loss: 0.61494 - acc: 0.8580 -- iter: 8/8\n",
      "--\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.56931\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 231 | loss: 0.56931 - acc: 0.8722 -- iter: 8/8\n",
      "--\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.52823\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 232 | loss: 0.52823 - acc: 0.8850 -- iter: 8/8\n",
      "--\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.49122\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 233 | loss: 0.49122 - acc: 0.8965 -- iter: 8/8\n",
      "--\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.45786\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 234 | loss: 0.45786 - acc: 0.9069 -- iter: 8/8\n",
      "--\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.42776\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 235 | loss: 0.42776 - acc: 0.9162 -- iter: 8/8\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.40060\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 236 | loss: 0.40060 - acc: 0.9246 -- iter: 8/8\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.37605\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 237 | loss: 0.37605 - acc: 0.9321 -- iter: 8/8\n",
      "--\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.54225\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 238 | loss: 0.54225 - acc: 0.8889 -- iter: 8/8\n",
      "--\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.50341\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 239 | loss: 0.50341 - acc: 0.9000 -- iter: 8/8\n",
      "--\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.46842\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 240 | loss: 0.46842 - acc: 0.9100 -- iter: 8/8\n",
      "--\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.43688\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 241 | loss: 0.43688 - acc: 0.9190 -- iter: 8/8\n",
      "--\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.40843\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 242 | loss: 0.40843 - acc: 0.9271 -- iter: 8/8\n",
      "--\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.38274\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 243 | loss: 0.38274 - acc: 0.9344 -- iter: 8/8\n",
      "--\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.56086\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 244 | loss: 0.56086 - acc: 0.8660 -- iter: 8/8\n",
      "--\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.51987\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 245 | loss: 0.51987 - acc: 0.8794 -- iter: 8/8\n",
      "--\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.48299\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 246 | loss: 0.48299 - acc: 0.8914 -- iter: 8/8\n",
      "--\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.44978\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 247 | loss: 0.44978 - acc: 0.9023 -- iter: 8/8\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.41986\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 248 | loss: 0.41986 - acc: 0.9120 -- iter: 8/8\n",
      "--\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.39288\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 249 | loss: 0.39288 - acc: 0.9208 -- iter: 8/8\n",
      "--\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.36853\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 250 | loss: 0.36853 - acc: 0.9288 -- iter: 8/8\n",
      "--\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.34654\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 251 | loss: 0.34654 - acc: 0.9359 -- iter: 8/8\n",
      "--\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.32665\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 252 | loss: 0.32665 - acc: 0.9423 -- iter: 8/8\n",
      "--\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.30865\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 253 | loss: 0.30865 - acc: 0.9481 -- iter: 8/8\n",
      "--\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.29233\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 254 | loss: 0.29233 - acc: 0.9533 -- iter: 8/8\n",
      "--\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.27753\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 255 | loss: 0.27753 - acc: 0.9579 -- iter: 8/8\n",
      "--\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.26407\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 256 | loss: 0.26407 - acc: 0.9621 -- iter: 8/8\n",
      "--\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.25183\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 257 | loss: 0.25183 - acc: 0.9659 -- iter: 8/8\n",
      "--\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.24067\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 258 | loss: 0.24067 - acc: 0.9693 -- iter: 8/8\n",
      "--\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.23049\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 259 | loss: 0.23049 - acc: 0.9724 -- iter: 8/8\n",
      "--\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.22118\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 260 | loss: 0.22118 - acc: 0.9752 -- iter: 8/8\n",
      "--\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.21265\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 261 | loss: 0.21265 - acc: 0.9776 -- iter: 8/8\n",
      "--\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.20482\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 262 | loss: 0.20482 - acc: 0.9799 -- iter: 8/8\n",
      "--\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.19763\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 263 | loss: 0.19763 - acc: 0.9819 -- iter: 8/8\n",
      "--\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.43067\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 264 | loss: 0.43067 - acc: 0.9087 -- iter: 8/8\n",
      "--\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.40071\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 265 | loss: 0.40071 - acc: 0.9178 -- iter: 8/8\n",
      "--\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.37370\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 266 | loss: 0.37370 - acc: 0.9260 -- iter: 8/8\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.34933\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 267 | loss: 0.34933 - acc: 0.9334 -- iter: 8/8\n",
      "--\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.32734\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 268 | loss: 0.32734 - acc: 0.9401 -- iter: 8/8\n",
      "--\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.30748\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 269 | loss: 0.30748 - acc: 0.9461 -- iter: 8/8\n",
      "--\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.28952\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 270 | loss: 0.28952 - acc: 0.9515 -- iter: 8/8\n",
      "--\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.27326\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 271 | loss: 0.27326 - acc: 0.9563 -- iter: 8/8\n",
      "--\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.25854\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 272 | loss: 0.25854 - acc: 0.9607 -- iter: 8/8\n",
      "--\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.24519\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 273 | loss: 0.24519 - acc: 0.9646 -- iter: 8/8\n",
      "--\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.23306\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 274 | loss: 0.23306 - acc: 0.9682 -- iter: 8/8\n",
      "--\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.22204\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 275 | loss: 0.22204 - acc: 0.9713 -- iter: 8/8\n",
      "--\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.21201\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 276 | loss: 0.21201 - acc: 0.9742 -- iter: 8/8\n",
      "--\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.20286\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 277 | loss: 0.20286 - acc: 0.9768 -- iter: 8/8\n",
      "--\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.19450\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 278 | loss: 0.19450 - acc: 0.9791 -- iter: 8/8\n",
      "--\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.18686\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 279 | loss: 0.18686 - acc: 0.9812 -- iter: 8/8\n",
      "--\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.17985\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 280 | loss: 0.17985 - acc: 0.9831 -- iter: 8/8\n",
      "--\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.16751\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 281 | loss: 0.16751 - acc: 0.9848 -- iter: 8/8\n",
      "--\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.16751\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 282 | loss: 0.16751 - acc: 0.9863 -- iter: 8/8\n",
      "--\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.16206\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 283 | loss: 0.16206 - acc: 0.9877 -- iter: 8/8\n",
      "--\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.15703\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 284 | loss: 0.15703 - acc: 0.9889 -- iter: 8/8\n",
      "--\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.15237\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 285 | loss: 0.15237 - acc: 0.9900 -- iter: 8/8\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.14805\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 286 | loss: 0.14805 - acc: 0.9910 -- iter: 8/8\n",
      "--\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.14403\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 287 | loss: 0.14403 - acc: 0.9919 -- iter: 8/8\n",
      "--\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.14029\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 288 | loss: 0.14029 - acc: 0.9927 -- iter: 8/8\n",
      "--\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.13680\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 289 | loss: 0.13680 - acc: 0.9934 -- iter: 8/8\n",
      "--\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.13353\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 290 | loss: 0.13353 - acc: 0.9941 -- iter: 8/8\n",
      "--\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.13047\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 291 | loss: 0.13047 - acc: 0.9947 -- iter: 8/8\n",
      "--\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.12758\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 292 | loss: 0.12758 - acc: 0.9952 -- iter: 8/8\n",
      "--\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.12487\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 293 | loss: 0.12487 - acc: 0.9957 -- iter: 8/8\n",
      "--\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.12230\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 294 | loss: 0.12230 - acc: 0.9961 -- iter: 8/8\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.11987\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 295 | loss: 0.11987 - acc: 0.9965 -- iter: 8/8\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.11757\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 296 | loss: 0.11757 - acc: 0.9969 -- iter: 8/8\n",
      "--\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.11538\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 297 | loss: 0.11538 - acc: 0.9972 -- iter: 8/8\n",
      "--\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.11329\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 298 | loss: 0.11329 - acc: 0.9975 -- iter: 8/8\n",
      "--\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.11130\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 299 | loss: 0.11130 - acc: 0.9977 -- iter: 8/8\n",
      "--\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.10939\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 300 | loss: 0.10939 - acc: 0.9979 -- iter: 8/8\n",
      "--\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.10757\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 301 | loss: 0.10757 - acc: 0.9981 -- iter: 8/8\n",
      "--\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.39292\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 302 | loss: 0.39292 - acc: 0.9233 -- iter: 8/8\n",
      "--\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.36263\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 303 | loss: 0.36263 - acc: 0.9310 -- iter: 8/8\n",
      "--\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.33536\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 304 | loss: 0.33536 - acc: 0.9379 -- iter: 8/8\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.31079\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 305 | loss: 0.31079 - acc: 0.9441 -- iter: 8/8\n",
      "--\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.28866\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 306 | loss: 0.28866 - acc: 0.9497 -- iter: 8/8\n",
      "--\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.26871\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 307 | loss: 0.26871 - acc: 0.9547 -- iter: 8/8\n",
      "--\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.46414\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 308 | loss: 0.46414 - acc: 0.9093 -- iter: 8/8\n",
      "--\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.42663\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 309 | loss: 0.42663 - acc: 0.9183 -- iter: 8/8\n",
      "--\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.39290\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 310 | loss: 0.39290 - acc: 0.9265 -- iter: 8/8\n",
      "--\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.36256\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 311 | loss: 0.36256 - acc: 0.9338 -- iter: 8/8\n",
      "--\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.33525\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 312 | loss: 0.33525 - acc: 0.9405 -- iter: 8/8\n",
      "--\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.31067\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 313 | loss: 0.31067 - acc: 0.9464 -- iter: 8/8\n",
      "--\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.28853\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 314 | loss: 0.28853 - acc: 0.9518 -- iter: 8/8\n",
      "--\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.26859\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 315 | loss: 0.26859 - acc: 0.9566 -- iter: 8/8\n",
      "--\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.25061\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 316 | loss: 0.25061 - acc: 0.9609 -- iter: 8/8\n",
      "--\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.23440\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 317 | loss: 0.23440 - acc: 0.9648 -- iter: 8/8\n",
      "--\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.21976\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 318 | loss: 0.21976 - acc: 0.9684 -- iter: 8/8\n",
      "--\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.20655\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 319 | loss: 0.20655 - acc: 0.9715 -- iter: 8/8\n",
      "--\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.19460\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 320 | loss: 0.19460 - acc: 0.9744 -- iter: 8/8\n",
      "--\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.18380\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 321 | loss: 0.18380 - acc: 0.9769 -- iter: 8/8\n",
      "--\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.35053\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 322 | loss: 0.35053 - acc: 0.9292 -- iter: 8/8\n",
      "--\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.32407\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 323 | loss: 0.32407 - acc: 0.9363 -- iter: 8/8\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.30024\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 324 | loss: 0.30024 - acc: 0.9427 -- iter: 8/8\n",
      "--\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.27877\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 325 | loss: 0.27877 - acc: 0.9484 -- iter: 8/8\n",
      "--\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.25942\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 326 | loss: 0.25942 - acc: 0.9536 -- iter: 8/8\n",
      "--\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.24198\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 327 | loss: 0.24198 - acc: 0.9582 -- iter: 8/8\n",
      "--\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.22624\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 328 | loss: 0.22624 - acc: 0.9624 -- iter: 8/8\n",
      "--\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.21203\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 329 | loss: 0.21203 - acc: 0.9662 -- iter: 8/8\n",
      "--\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.19919\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 330 | loss: 0.19919 - acc: 0.9695 -- iter: 8/8\n",
      "--\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.18759\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 331 | loss: 0.18759 - acc: 0.9726 -- iter: 8/8\n",
      "--\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.17710\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 332 | loss: 0.17710 - acc: 0.9753 -- iter: 8/8\n",
      "--\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.16760\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 333 | loss: 0.16760 - acc: 0.9778 -- iter: 8/8\n",
      "--\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.15900\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 334 | loss: 0.15900 - acc: 0.9800 -- iter: 8/8\n",
      "--\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.15119\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 335 | loss: 0.15119 - acc: 0.9820 -- iter: 8/8\n",
      "--\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.14411\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 336 | loss: 0.14411 - acc: 0.9838 -- iter: 8/8\n",
      "--\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.13767\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 337 | loss: 0.13767 - acc: 0.9854 -- iter: 8/8\n",
      "--\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.35902\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 338 | loss: 0.35902 - acc: 0.9244 -- iter: 8/8\n",
      "--\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.33103\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 339 | loss: 0.33103 - acc: 0.9319 -- iter: 8/8\n",
      "--\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.30585\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 340 | loss: 0.30585 - acc: 0.9388 -- iter: 8/8\n",
      "--\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.28318\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 341 | loss: 0.28318 - acc: 0.9449 -- iter: 8/8\n",
      "--\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.26277\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 342 | loss: 0.26277 - acc: 0.9504 -- iter: 8/8\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.24438\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 343 | loss: 0.24438 - acc: 0.9554 -- iter: 8/8\n",
      "--\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.22780\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 344 | loss: 0.22780 - acc: 0.9598 -- iter: 8/8\n",
      "--\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.21286\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 345 | loss: 0.21286 - acc: 0.9638 -- iter: 8/8\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\Tom\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.fit(training, output, n_epoch=345, batch_size=8, show_metric=True)\n",
    "model.save(\"model.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start talking with the bot (type quit to stop)!\n",
      "Listening...\n",
      "where am I\n",
      "You are at the Parthenon, a former temple on the Athenian Acropolis in Greece dedicated to the goddess Athena.\n",
      "Listening...\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    return numpy.array(bag)\n",
    "\n",
    "\n",
    "def chat():\n",
    "        print(\"Start talking with the bot (type quit to stop)!\")\n",
    "        while True:\n",
    "            not_crashed = True\n",
    "            with sr.Microphone() as source:\n",
    "                print(\"Listening...\")\n",
    "                engine.say(\"Listening\")\n",
    "                engine.runAndWait()\n",
    "                try:\n",
    "                    inp = r.listen(source)\n",
    "                    inp = r.recognize_google(inp)\n",
    "                    print(inp)\n",
    "                except:\n",
    "                    print(\"Please try again\")\n",
    "                    not_crashed = False\n",
    "            if not_crashed:     \n",
    "                if inp.lower() == \"quit\":\n",
    "                    break\n",
    "\n",
    "                results = model.predict([bag_of_words(inp, words)])\n",
    "                results_index = numpy.argmax(results)\n",
    "                tag = labels[results_index]\n",
    "\n",
    "                for tg in data[\"intents\"]:\n",
    "                    if tg['tag'] == tag:\n",
    "                        responses = tg['responses']\n",
    "\n",
    "                print(random.choice(responses))\n",
    "                engine.say(random.choice(responses))\n",
    "                engine.runAndWait()\n",
    "                \n",
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
